{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a simplified implementation of the GradientDescentOptimizer in tensorflow using general backpropagation. Code is referenced from the Tensorflow source code, specifically, `gradients.py` and `math_grad.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('data_dir', '/tmp/data/', 'Directory for storing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import gen_array_ops\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, epoch=1000, learning_rate=0.5, batch_size=100):\n",
    "        self.epoch = epoch\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.W1 = None\n",
    "        self.b1 = None\n",
    "    \n",
    "    def fit(self, train, use_tf=False):\n",
    "        batch_X, batch_y = train.next_batch(self.batch_size)\n",
    "        d = batch_X.shape[1]\n",
    "        c = batch_y.shape[1]\n",
    "\n",
    "        tf_X = tf.placeholder(tf.float32, [None, d])\n",
    "        tf_y = tf.placeholder(tf.float32, [None, c])\n",
    "        self.W1 = tf.Variable(tf.random_normal([d, c], seed=1)/d)\n",
    "        self.b1 = tf.Variable(tf.zeros(c))\n",
    "        z = tf.matmul(tf_X, self.W1) + self.b1\n",
    "        y_hat = tf.nn.softmax(z)\n",
    "        cross_entropy = tf.reduce_mean(-tf.reduce_sum(tf_y * tf.log(y_hat), reduction_indices=[1]))\n",
    "\n",
    "        sess.run(tf.initialize_variables([self.W1, self.b1]))\n",
    "\n",
    "        if use_tf:\n",
    "            steps = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(cross_entropy)\n",
    "        else:\n",
    "            steps = self.train_step(cross_entropy)\n",
    "\n",
    "        for i in range(self.epoch):\n",
    "            if i != 0:\n",
    "                batch_X, batch_y = train.next_batch(self.batch_size)\n",
    "            sess.run(steps, feed_dict={tf_X: batch_X, tf_y: batch_y})\n",
    "\n",
    "    def predict(self, X):\n",
    "        tf_X = tf.placeholder(tf.float32, [None, X.shape[1]])\n",
    "        y_hat = tf.nn.softmax(tf.matmul(tf_X, self.W1) + self.b1)\n",
    "\n",
    "        return sess.run(y_hat, feed_dict={tf_X: X})\n",
    "\n",
    "    def var_list(self):\n",
    "        return [self.W1, self.b1]\n",
    "\n",
    "    def train_step(self, cost):\n",
    "        grads_and_vars = self.gradients(cost)\n",
    "        return [tf.assign(var, var - self.learning_rate * grad) for grad, var in grads_and_vars]\n",
    "\n",
    "    def gradients(self, cost):\n",
    "        ''' Ref: Deep Learning Book: 6.5.6 General back-propagation; Tensorflow paper: 4.1 Gradient computation\n",
    "        \n",
    "        Starting from cost output, use chain rule to back propagate gradients to variables.\n",
    "        '''\n",
    "        op = cost.op\n",
    "        # grads store the output gradient of an operation\n",
    "        grads = {op: self.default_gradient(op)}\n",
    "        queue = deque()\n",
    "        self._append_op_to_queue(queue, op)\n",
    "        while queue:\n",
    "            op = queue.popleft()\n",
    "            # extract the gradient function to calculate the input gradients by applying chain rule\n",
    "            grad_fn = self._grad_fn(op)\n",
    "            in_grads = grad_fn(op, grads[op])\n",
    "            for t_in, in_grad in zip(op.inputs, in_grads):\n",
    "                grads[t_in.op] = in_grad\n",
    "                self._append_op_to_queue(queue, t_in.op)\n",
    "\n",
    "        return [(grads[xs.op], xs) for xs in self.var_list()]\n",
    "\n",
    "    def default_gradient(self, op):\n",
    "        out = op.outputs[0]\n",
    "        return tf.fill(tf.shape(out), tf.constant(1, dtype=out.dtype))\n",
    "\n",
    "    def _append_op_to_queue(self, queue, op):\n",
    "        if len(op.inputs) > 0:\n",
    "            queue.append(op)\n",
    "\n",
    "    def _grad_fn(self, op):\n",
    "        if op.type == 'MatMul':\n",
    "            return self._matmul_grad\n",
    "        elif op.type == 'Add':\n",
    "            return self._add_grad\n",
    "        elif op.type == 'Identity':\n",
    "            return self._id_grad\n",
    "        elif op.type == 'Softmax':\n",
    "            return self._softmax_grad\n",
    "        elif op.type == 'Log':\n",
    "            return self._log_grad\n",
    "        elif op.type == 'Mul':\n",
    "            return self._mul_grad\n",
    "        elif op.type == 'Sum':\n",
    "            return self._sum_grad\n",
    "        elif op.type == 'Neg':\n",
    "            return self._neg_grad\n",
    "        elif op.type == 'Mean':\n",
    "            return self._mean_grad\n",
    "\n",
    "    def _add_grad(self, op, grad):\n",
    "        x = op.inputs[0]\n",
    "        y = op.inputs[1]\n",
    "        sx = tf.shape(x)\n",
    "        sy = tf.shape(y)\n",
    "        rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n",
    "        return (\n",
    "            tf.reshape(tf.reduce_sum(grad, rx), sx),\n",
    "            tf.reshape(tf.reduce_sum(grad, ry), sy)\n",
    "        )\n",
    "\n",
    "    def _matmul_grad(self, op, grad):\n",
    "        x = op.inputs[0]\n",
    "        y = op.inputs[1]\n",
    "        return (tf.matmul(grad, y, transpose_b=True), tf.matmul(x, grad, transpose_a=True))\n",
    "\n",
    "    def _id_grad(self, op, grad):\n",
    "        return (grad,)\n",
    "\n",
    "    def _softmax_grad(self, op, grad):\n",
    "        softmax = op.outputs[0]\n",
    "        return ((grad - tf.reshape(tf.reduce_sum(grad * softmax, [1]), [-1, 1])) * softmax,)\n",
    "\n",
    "    def _log_grad(self, op, grad):\n",
    "        x = op.inputs[0]\n",
    "        return (grad * tf.inv(x),)\n",
    "\n",
    "    def _mul_grad(self, op, grad):\n",
    "        x = op.inputs[0]\n",
    "        y = op.inputs[1]\n",
    "        sx = tf.shape(x)\n",
    "        sy = tf.shape(y)\n",
    "        rx, ry = gen_array_ops._broadcast_gradient_args(sx, sy)\n",
    "        return (\n",
    "            tf.reshape(tf.reduce_sum(grad * y, rx), sx),\n",
    "            tf.reshape(tf.reduce_sum(grad * x, ry), sy)\n",
    "        )\n",
    "\n",
    "    def _sum_grad(self, op, grad):\n",
    "        input_shape = tf.shape(op.inputs[0])\n",
    "        axes = op.inputs[1]\n",
    "        reduced_shape = self._reduced_shape(input_shape, axes)\n",
    "        tile_scaling = input_shape // reduced_shape\n",
    "        return (tf.tile(tf.reshape(grad, reduced_shape), tile_scaling),)\n",
    "\n",
    "    def _neg_grad(self, op, grad):\n",
    "        return (-grad,)\n",
    "\n",
    "    def _mean_grad(self, op, grad):\n",
    "        sum_grad = self._sum_grad(op, grad)[0]\n",
    "        input_shape = tf.shape(op.inputs[0])\n",
    "        output_shape = tf.shape(op.outputs[0])\n",
    "        factor = tf.reduce_prod(input_shape) // tf.reduce_prod(output_shape)\n",
    "        return (sum_grad / tf.cast(factor, sum_grad.dtype),)\n",
    "\n",
    "    def _reduced_shape(self, input_shape, axes):\n",
    "        '''\n",
    "        For example: tf.reduce_sum(input, axes): input shape [2, 5, 3, 7], axes = [1, 2]\n",
    "        Then reduced_shape is [2, 1, 1, 7]\n",
    "        '''\n",
    "        input_rank = tf.size(input_shape)\n",
    "        axes = (axes + input_rank) % input_rank\n",
    "        return tf.dynamic_stitch(\n",
    "            [tf.range(input_rank), axes],\n",
    "            [input_shape, tf.fill(tf.shape(axes), 1)]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow General Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = NeuralNet(epoch=1000, batch_size=100, learning_rate=0.5)\n",
    "nn.fit(mnist.train, use_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91620000000000001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = nn.predict(mnist.test.images)\n",
    "accuracy_score(np.argmax(mnist.test.labels, 1), np.argmax(y_hat, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual General Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91790000000000005"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNet(epoch=1000, batch_size=100, learning_rate=0.5)\n",
    "nn.fit(mnist.train, use_tf=False)\n",
    "\n",
    "y_hat = nn.predict(mnist.test.images)\n",
    "accuracy_score(np.argmax(mnist.test.labels, 1), np.argmax(y_hat, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
